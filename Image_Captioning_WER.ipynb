{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Image Captioning with WER as a metric"
      ],
      "metadata": {
        "id": "-AngmxznaZQL"
      },
      "id": "-AngmxznaZQL"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Mount Google Drive for the caption and images to train on."
      ],
      "metadata": {
        "id": "SKa4TKvTagcf"
      },
      "id": "SKa4TKvTagcf"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WNH_w6nrYjSW",
        "outputId": "2e27fb98-3d7e-4491-8e78-36bcf92c3791"
      },
      "id": "WNH_w6nrYjSW",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install jiwer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lsFq-gO5FUC_",
        "outputId": "b6455e20-d2c1-4831-c93d-6218a7f10b5f"
      },
      "id": "lsFq-gO5FUC_",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting jiwer\n",
            "  Downloading jiwer-3.0.4-py3-none-any.whl (21 kB)\n",
            "Requirement already satisfied: click<9.0.0,>=8.1.3 in /usr/local/lib/python3.10/dist-packages (from jiwer) (8.1.7)\n",
            "Collecting rapidfuzz<4,>=3 (from jiwer)\n",
            "  Downloading rapidfuzz-3.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rapidfuzz, jiwer\n",
            "Successfully installed jiwer-3.0.4 rapidfuzz-3.9.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout, add\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import json\n",
        "import os\n",
        "import jiwer\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "IaIykdA-cQs4",
        "outputId": "2dda873c-faf8-405e-d36a-62665de96d62",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "IaIykdA-cQs4",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if using GPU\n",
        "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mMc1L27Pf9_N",
        "outputId": "5741f2a0-0ece-4807-d0a6-ca8bacdb30dc"
      },
      "id": "mMc1L27Pf9_N",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num GPUs Available:  1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Read the caption file from drive and load in dataframe."
      ],
      "metadata": {
        "id": "sxsK8ec3cbUO"
      },
      "id": "sxsK8ec3cbUO"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the path to your JSON file\n",
        "json_file_path = '/content/drive/MyDrive/NLP/output_json.json'\n",
        "\n",
        "# Read the JSON file into a DataFrame\n",
        "caption_df = pd.read_json(json_file_path, orient='records', lines=True)\n",
        "\n",
        "# Print the DataFrame to verify\n",
        "print(caption_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ezJCvviNauLp",
        "outputId": "2212a8df-5e91-486d-b7d0-017b45ea9592"
      },
      "id": "ezJCvviNauLp",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        image/key                                                IIW  \\\n",
            "0  aar_test_04600  A close-up outdoor shot shows an Echinops Bann...   \n",
            "1  aar_test_04601  In a low-angle shot, a wall decorated with a p...   \n",
            "2  aar_test_04602  In an eye-level shot, a brick wall extends fro...   \n",
            "3  aar_test_04603  A close-up view of a brick wall reveals the wo...   \n",
            "4  aar_test_04604  A broad, pale-hued shelf encompasses a large m...   \n",
            "\n",
            "                                       processed_IIW  \\\n",
            "0  a closeup outdoor shot shows an echinops banna...   \n",
            "1  in a lowangle shot a wall decorated with a pat...   \n",
            "2  in an eyelevel shot a brick wall extends from ...   \n",
            "3  a closeup view of a brick wall reveals the wor...   \n",
            "4  a broad palehued shelf encompasses a large mat...   \n",
            "\n",
            "                                         summary_IIW  \n",
            "0  a closeup outdoor shot shows an echinops banna...  \n",
            "1  a wall decorated with a pattern of square tile...  \n",
            "2  brick wall extends from the foreground on the ...  \n",
            "3  a closeup view of a brick wall reveals the wor...  \n",
            "4  the brand samsung is subtly displayed in white...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Read the images from drive and load to the same dataframe"
      ],
      "metadata": {
        "id": "ycICr4mJdQuW"
      },
      "id": "ycICr4mJdQuW"
    },
    {
      "cell_type": "code",
      "source": [
        "# directory where images are stored\n",
        "image_dir = '/content/drive/MyDrive/NLP/Image_set/'\n",
        "\n",
        "# Function to load and preprocess images\n",
        "def load_and_preprocess_image(image_key):\n",
        "    image_path = os.path.join(image_dir, f\"{image_key}.jpg\")\n",
        "    image = load_img(image_path, target_size=(224, 224))   ## preprocessing, change size to 224x224\n",
        "    image = img_to_array(image)\n",
        "    image = np.expand_dims(image, axis=0)\n",
        "    image = preprocess_input(image)\n",
        "    return image\n",
        "\n",
        "# Load and preprocess all images\n",
        "caption_df['image_data'] = caption_df['image/key'].apply(load_and_preprocess_image)\n"
      ],
      "metadata": {
        "id": "WuLvNmRLdsEl"
      },
      "id": "WuLvNmRLdsEl",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extracting features from image using VGG16-ImageNet."
      ],
      "metadata": {
        "id": "CgcMa5CFgvYW"
      },
      "id": "CgcMa5CFgvYW"
    },
    {
      "cell_type": "code",
      "source": [
        "# Load VGG16 model pre-trained on ImageNet\n",
        "vgg_model = VGG16(weights='imagenet')\n",
        "vgg_model = Model(inputs=vgg_model.inputs, outputs=vgg_model.layers[-2].output)\n",
        "\n",
        "# Function to extract features using VGG16\n",
        "def extract_features(image_data):\n",
        "    features = vgg_model.predict(image_data, verbose=0)\n",
        "    return features\n",
        "\n",
        "# Extract features for all images\n",
        "caption_df['image_features'] = caption_df['image_data'].apply(extract_features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mh0WxhiTg6WS",
        "outputId": "83e8422c-6702-4606-d727-fc992c7e7455"
      },
      "id": "Mh0WxhiTg6WS",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels.h5\n",
            "553467096/553467096 [==============================] - 3s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing the Caption data"
      ],
      "metadata": {
        "id": "JdfYxZF6iiam"
      },
      "id": "JdfYxZF6iiam"
    },
    {
      "cell_type": "code",
      "source": [
        "## Loading stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "## Removing stopwords\n",
        "def remove_stop_words(text):\n",
        "    tokens = text.split()\n",
        "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "    return ' '.join(filtered_tokens)\n",
        "\n",
        "# Apply the function to the 'summary_IIW' column\n",
        "caption_df['filtered_summary_IIW'] = caption_df['summary_IIW'].apply(remove_stop_words)"
      ],
      "metadata": {
        "id": "nO0QI7LMpo1Q"
      },
      "id": "nO0QI7LMpo1Q",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the captions\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(caption_df['filtered_summary_IIW'])\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Convert captions to sequences\n",
        "sequences = tokenizer.texts_to_sequences(caption_df['filtered_summary_IIW'])\n",
        "\n",
        "# Pad sequences\n",
        "max_length = max(len(seq) for seq in sequences)\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')"
      ],
      "metadata": {
        "id": "-NSzMlj4ipUC"
      },
      "id": "-NSzMlj4ipUC",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split Data into Train and Test Sets"
      ],
      "metadata": {
        "id": "RNIf2QEni9Ef"
      },
      "id": "RNIf2QEni9Ef"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Prepare features and captions arrays\n",
        "features = np.vstack(caption_df['image_features'])\n",
        "captions = padded_sequences\n",
        "\n",
        "# Split data into training and testing sets\n",
        "train_features, test_features, train_captions, test_captions = train_test_split(features, captions, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "UIyBe_zYi_Bi"
      },
      "id": "UIyBe_zYi_Bi",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Image Captioning Model"
      ],
      "metadata": {
        "id": "NbZuXoogjTMw"
      },
      "id": "NbZuXoogjTMw"
    },
    {
      "cell_type": "code",
      "source": [
        "# Image captioning model\n",
        "# Image feature extractor\n",
        "image_input = Input(shape=(4096,))\n",
        "image_dense = Dropout(0.5)(image_input)\n",
        "image_dense = Dense(256, activation='relu')(image_dense)\n",
        "\n",
        "# Sequence processor\n",
        "caption_input = Input(shape=(max_length,))\n",
        "caption_embedding = Embedding(vocab_size, 256, mask_zero=True)(caption_input)\n",
        "caption_lstm = Dropout(0.5)(caption_embedding)\n",
        "caption_lstm = LSTM(256)(caption_lstm)\n",
        "\n",
        "# Decoder (feed both outputs into a single model)\n",
        "decoder = add([image_dense, caption_lstm])\n",
        "decoder = Dense(256, activation='relu')(decoder)\n",
        "outputs = Dense(vocab_size, activation='softmax')(decoder)\n",
        "\n",
        "# Compile the model\n",
        "model = Model(inputs=[image_input, caption_input], outputs=outputs)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8j5ZhEq7jWI3",
        "outputId": "9bc9451d-514c-4756-d40d-eb05eb67b3bd"
      },
      "id": "8j5ZhEq7jWI3",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)        [(None, 65)]                 0         []                            \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)        [(None, 4096)]               0         []                            \n",
            "                                                                                                  \n",
            " embedding (Embedding)       (None, 65, 256)              976640    ['input_3[0][0]']             \n",
            "                                                                                                  \n",
            " dropout (Dropout)           (None, 4096)                 0         ['input_2[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)         (None, 65, 256)              0         ['embedding[0][0]']           \n",
            "                                                                                                  \n",
            " dense (Dense)               (None, 256)                  1048832   ['dropout[0][0]']             \n",
            "                                                                                                  \n",
            " lstm (LSTM)                 (None, 256)                  525312    ['dropout_1[0][0]']           \n",
            "                                                                                                  \n",
            " add (Add)                   (None, 256)                  0         ['dense[0][0]',               \n",
            "                                                                     'lstm[0][0]']                \n",
            "                                                                                                  \n",
            " dense_1 (Dense)             (None, 256)                  65792     ['add[0][0]']                 \n",
            "                                                                                                  \n",
            " dense_2 (Dense)             (None, 3815)                 980455    ['dense_1[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 3597031 (13.72 MB)\n",
            "Trainable params: 3597031 (13.72 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train the Model"
      ],
      "metadata": {
        "id": "qq5dbv62khn2"
      },
      "id": "qq5dbv62khn2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Define the parameters"
      ],
      "metadata": {
        "id": "9noYOGB9kmvX"
      },
      "id": "9noYOGB9kmvX"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define parameters\n",
        "batch_size = 32\n",
        "epochs = 20\n",
        "steps = len(train_features) // batch_size"
      ],
      "metadata": {
        "id": "g8gJ2lbakv7u"
      },
      "id": "g8gJ2lbakv7u",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Creating data generators"
      ],
      "metadata": {
        "id": "5RiXD7Awk0XQ"
      },
      "id": "5RiXD7Awk0XQ"
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare data generators\n",
        "def data_generator(features, captions, batch_size):\n",
        "    while True:\n",
        "        for i in range(0, len(features), batch_size):\n",
        "            batch_features = features[i:i+batch_size]\n",
        "            batch_captions = captions[i:i+batch_size]\n",
        "            X1, X2, y = [], [], []\n",
        "            for j in range(len(batch_features)):\n",
        "                seq = batch_captions[j]\n",
        "                for k in range(1, len(seq)):\n",
        "                    in_seq, out_seq = seq[:k], seq[k]\n",
        "                    in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
        "                    out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
        "                    X1.append(batch_features[j])\n",
        "                    X2.append(in_seq)\n",
        "                    y.append(out_seq)\n",
        "            yield [np.array(X1), np.array(X2)], np.array(y)"
      ],
      "metadata": {
        "id": "GJK9o6bdk8yZ"
      },
      "id": "GJK9o6bdk8yZ",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create train-test data generators\n",
        "train_generator = data_generator(train_features, train_captions, batch_size)\n",
        "test_generator = data_generator(test_features, test_captions, batch_size)"
      ],
      "metadata": {
        "id": "P58kwhTVlF2l"
      },
      "id": "P58kwhTVlF2l",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Fit"
      ],
      "metadata": {
        "id": "yC9JtOkXldYy"
      },
      "id": "yC9JtOkXldYy"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Introducing Word Error Rate (WER) into the training loop to fine tune the model.\n",
        "Using WER as a metric to fine-tune an image captioning model helps ensure that the generated captions are as close as possible to the ground-truth captions. By integrating WER into the training loop, you can dynamically adjust the training process to improve model performance, achieve better generalization, and avoid overfitting. This process involves calculating WER for validation data, adjusting learning rates, implementing early stopping, and saving the best model based on WER scores."
      ],
      "metadata": {
        "id": "Amhsw7lnL4dm"
      },
      "id": "Amhsw7lnL4dm"
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_caption(model, tokenizer, photo, max_length):\n",
        "    in_text = 'startseq'\n",
        "    for _ in range(max_length):\n",
        "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
        "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
        "        yhat = model.predict([photo, sequence], verbose=0)\n",
        "        yhat = np.argmax(yhat)\n",
        "        word = tokenizer.index_word.get(yhat)\n",
        "        if word is None:\n",
        "            break\n",
        "        in_text += ' ' + word\n",
        "        if word == 'endseq':\n",
        "            break\n",
        "    final_caption = in_text.split()[1:-1]\n",
        "    final_caption = ' '.join(final_caption)\n",
        "    return final_caption"
      ],
      "metadata": {
        "id": "PXkRU36CPCjL"
      },
      "id": "PXkRU36CPCjL",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Implement a function to calculate WER between generated and reference captions.\n",
        "def calculate_wer(reference, hypothesis):\n",
        "    return jiwer.wer(reference, hypothesis)\n",
        "\n",
        "def evaluate_model(model, tokenizer, test_features, test_captions, max_length):\n",
        "    actual, predicted = list(), list()\n",
        "    for img_features, caption in zip(test_features, test_captions):\n",
        "        img_features = np.expand_dims(img_features, axis=0)  # Ensure the correct shape\n",
        "        yhat = generate_caption(model, tokenizer, img_features, max_length)\n",
        "        actual_caption = ' '.join([tokenizer.index_word[i] for i in caption if i > 0])\n",
        "        actual.append(actual_caption)\n",
        "        predicted.append(yhat)\n",
        "    # Calculate WER for all predictions\n",
        "    wer_scores = [calculate_wer(act, pred) for act, pred in zip(actual, predicted)]\n",
        "    return sum(wer_scores) / len(wer_scores)\n",
        "\n",
        "# Define the image captioning model, assuming it's already defined and compiled\n",
        "\n",
        "# Training loop\n",
        "best_wer = float('inf')\n",
        "patience = 5\n",
        "patience_counter = 0\n"
      ],
      "metadata": {
        "id": "979i786yMSyJ"
      },
      "id": "979i786yMSyJ",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(epochs):\n",
        "    model.fit(train_generator, epochs=1, steps_per_epoch=steps, validation_data=test_generator, validation_steps=len(test_features) // batch_size)\n",
        "\n",
        "    # Calculate WER on the validation set\n",
        "    current_wer = evaluate_model(model, tokenizer, test_features, test_captions, max_length)\n",
        "    print(f'Epoch {epoch+1}, WER: {current_wer}')\n",
        "\n",
        "    # Check if the WER has improved\n",
        "    if current_wer < best_wer:\n",
        "        best_wer = current_wer\n",
        "        model.save('best_model.h5')\n",
        "        patience_counter = 0\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "\n",
        "    # Early stopping\n",
        "    print(\"patience_counter: \", patience_counter)\n",
        "    if patience_counter >= patience:\n",
        "        print('Early stopping triggered')\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "clZ1bV4kMnDB",
        "outputId": "db3c12c3-8f17-4d7c-9efb-f9a10cdc63d0"
      },
      "id": "clZ1bV4kMnDB",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10/10 [==============================] - 14s 571ms/step - loss: 6.6138 - val_loss: 5.7503\n",
            "Epoch 1, WER: 1.0\n",
            "patience_counter:  0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10/10 [==============================] - 4s 361ms/step - loss: 5.7035 - val_loss: 5.9322\n",
            "Epoch 2, WER: 1.0\n",
            "patience_counter:  1\n",
            "10/10 [==============================] - 4s 355ms/step - loss: 5.5194 - val_loss: 5.6403\n",
            "Epoch 3, WER: 1.0\n",
            "patience_counter:  2\n",
            "10/10 [==============================] - 4s 369ms/step - loss: 5.2119 - val_loss: 5.3977\n",
            "Epoch 4, WER: 1.0\n",
            "patience_counter:  3\n",
            "10/10 [==============================] - 4s 406ms/step - loss: 4.9654 - val_loss: 5.3542\n",
            "Epoch 5, WER: 1.0\n",
            "patience_counter:  4\n",
            "10/10 [==============================] - 5s 453ms/step - loss: 4.8394 - val_loss: 5.4319\n",
            "Epoch 6, WER: 0.9994565217391305\n",
            "patience_counter:  0\n",
            "10/10 [==============================] - 4s 350ms/step - loss: 4.7040 - val_loss: 5.2996\n",
            "Epoch 7, WER: 1.0146932594463816\n",
            "patience_counter:  1\n",
            "10/10 [==============================] - 4s 421ms/step - loss: 4.4702 - val_loss: 5.4975\n",
            "Epoch 8, WER: 0.9981291806020067\n",
            "patience_counter:  0\n",
            "10/10 [==============================] - 4s 376ms/step - loss: 4.3528 - val_loss: 5.4846\n",
            "Epoch 9, WER: 0.9990049433785131\n",
            "patience_counter:  1\n",
            "10/10 [==============================] - 5s 481ms/step - loss: 4.0391 - val_loss: 5.6885\n",
            "Epoch 10, WER: 0.9967820483170854\n",
            "patience_counter:  0\n",
            "10/10 [==============================] - 4s 356ms/step - loss: 3.8093 - val_loss: 5.8946\n",
            "Epoch 11, WER: 1.0082535777012382\n",
            "patience_counter:  1\n",
            "10/10 [==============================] - 5s 494ms/step - loss: 3.6339 - val_loss: 5.9111\n",
            "Epoch 12, WER: 1.039985651887472\n",
            "patience_counter:  2\n",
            "10/10 [==============================] - 4s 373ms/step - loss: 3.4871 - val_loss: 5.9852\n",
            "Epoch 13, WER: 1.0377153171855196\n",
            "patience_counter:  3\n",
            "10/10 [==============================] - 4s 376ms/step - loss: 3.4375 - val_loss: 5.8890\n",
            "Epoch 14, WER: 1.2341906971052257\n",
            "patience_counter:  4\n",
            "10/10 [==============================] - 4s 373ms/step - loss: 3.2520 - val_loss: 5.9983\n",
            "Epoch 15, WER: 1.1015475019529126\n",
            "patience_counter:  5\n",
            "Early stopping triggered\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save the trained model and tokenizer"
      ],
      "metadata": {
        "id": "XgVT2F-xp5yc"
      },
      "id": "XgVT2F-xp5yc"
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the trained model\n",
        "model.save('/content/drive/MyDrive/NLP/new_model.h5')"
      ],
      "metadata": {
        "id": "E3LjOv62Vkrr"
      },
      "id": "E3LjOv62Vkrr",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the tokenizer to a JSON file\n",
        "tokenizer_json = tokenizer.to_json()\n",
        "with open('/content/drive/MyDrive/NLP/new_tokenizer.json', 'w') as json_file:\n",
        "    json_file.write(tokenizer_json)"
      ],
      "metadata": {
        "id": "unB7mbEMr-Yv"
      },
      "id": "unB7mbEMr-Yv",
      "execution_count": 21,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}