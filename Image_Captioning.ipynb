{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Image Captioning"
      ],
      "metadata": {
        "id": "-AngmxznaZQL"
      },
      "id": "-AngmxznaZQL"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Mount Google Drive for the caption and images to train on."
      ],
      "metadata": {
        "id": "SKa4TKvTagcf"
      },
      "id": "SKa4TKvTagcf"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WNH_w6nrYjSW",
        "outputId": "aa04eb75-3f22-436f-e49d-5307fc57742c"
      },
      "id": "WNH_w6nrYjSW",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout, add\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import json\n",
        "import os"
      ],
      "metadata": {
        "id": "IaIykdA-cQs4"
      },
      "id": "IaIykdA-cQs4",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if using GPU\n",
        "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mMc1L27Pf9_N",
        "outputId": "0d1b9835-d29a-495a-8ca1-fa9bdae1e92d"
      },
      "id": "mMc1L27Pf9_N",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num GPUs Available:  1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Read the caption file from drive and load in dataframe."
      ],
      "metadata": {
        "id": "sxsK8ec3cbUO"
      },
      "id": "sxsK8ec3cbUO"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the path to your JSON file\n",
        "json_file_path = '/content/drive/MyDrive/NLP/output_json.json'\n",
        "\n",
        "# Read the JSON file into a DataFrame\n",
        "caption_df = pd.read_json(json_file_path, orient='records', lines=True)\n",
        "\n",
        "# Print the DataFrame to verify\n",
        "print(caption_df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ezJCvviNauLp",
        "outputId": "f3ed4c6d-4600-4774-819f-fe4517e1b828"
      },
      "id": "ezJCvviNauLp",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        image/key                                                IIW  \\\n",
            "0  aar_test_04600  A close-up outdoor shot shows an Echinops Bann...   \n",
            "1  aar_test_04601  In a low-angle shot, a wall decorated with a p...   \n",
            "2  aar_test_04602  In an eye-level shot, a brick wall extends fro...   \n",
            "3  aar_test_04603  A close-up view of a brick wall reveals the wo...   \n",
            "4  aar_test_04604  A broad, pale-hued shelf encompasses a large m...   \n",
            "\n",
            "                                       processed_IIW  \\\n",
            "0  a closeup outdoor shot shows an echinops banna...   \n",
            "1  in a lowangle shot a wall decorated with a pat...   \n",
            "2  in an eyelevel shot a brick wall extends from ...   \n",
            "3  a closeup view of a brick wall reveals the wor...   \n",
            "4  a broad palehued shelf encompasses a large mat...   \n",
            "\n",
            "                                         summary_IIW  \n",
            "0  a closeup outdoor shot shows an echinops banna...  \n",
            "1  a wall decorated with a pattern of square tile...  \n",
            "2  brick wall extends from the foreground on the ...  \n",
            "3  a closeup view of a brick wall reveals the wor...  \n",
            "4  the brand samsung is subtly displayed in white...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Read the images from drive and load to the same dataframe"
      ],
      "metadata": {
        "id": "ycICr4mJdQuW"
      },
      "id": "ycICr4mJdQuW"
    },
    {
      "cell_type": "code",
      "source": [
        "# directory where images are stored\n",
        "image_dir = '/content/drive/MyDrive/NLP/Image_set/'\n",
        "\n",
        "# Function to load and preprocess images\n",
        "def load_and_preprocess_image(image_key):\n",
        "    image_path = os.path.join(image_dir, f\"{image_key}.jpg\")\n",
        "    image = load_img(image_path, target_size=(224, 224))   ## preprocessing, change size to 224x224\n",
        "    image = img_to_array(image)\n",
        "    image = np.expand_dims(image, axis=0)\n",
        "    image = preprocess_input(image)\n",
        "    return image\n",
        "\n",
        "# Load and preprocess all images\n",
        "caption_df['image_data'] = caption_df['image/key'].apply(load_and_preprocess_image)\n"
      ],
      "metadata": {
        "id": "WuLvNmRLdsEl"
      },
      "id": "WuLvNmRLdsEl",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extracting features from image using VGG16-ImageNet."
      ],
      "metadata": {
        "id": "CgcMa5CFgvYW"
      },
      "id": "CgcMa5CFgvYW"
    },
    {
      "cell_type": "code",
      "source": [
        "# Load VGG16 model pre-trained on ImageNet\n",
        "vgg_model = VGG16(weights='imagenet')\n",
        "vgg_model = Model(inputs=vgg_model.inputs, outputs=vgg_model.layers[-2].output)\n",
        "\n",
        "# Function to extract features using VGG16\n",
        "def extract_features(image_data):\n",
        "    features = vgg_model.predict(image_data, verbose=0)\n",
        "    return features\n",
        "\n",
        "# Extract features for all images\n",
        "caption_df['image_features'] = caption_df['image_data'].apply(extract_features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mh0WxhiTg6WS",
        "outputId": "3b2e0643-457c-40e7-c680-fbfd90284bb7"
      },
      "id": "Mh0WxhiTg6WS",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels.h5\n",
            "553467096/553467096 [==============================] - 14s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing the Caption data"
      ],
      "metadata": {
        "id": "JdfYxZF6iiam"
      },
      "id": "JdfYxZF6iiam"
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the captions\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(caption_df['summary_IIW'])\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Convert captions to sequences\n",
        "sequences = tokenizer.texts_to_sequences(caption_df['summary_IIW'])\n",
        "\n",
        "# Pad sequences\n",
        "max_length = max(len(seq) for seq in sequences)\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')"
      ],
      "metadata": {
        "id": "-NSzMlj4ipUC"
      },
      "id": "-NSzMlj4ipUC",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split Data into Train and Test Sets"
      ],
      "metadata": {
        "id": "RNIf2QEni9Ef"
      },
      "id": "RNIf2QEni9Ef"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Prepare features and captions arrays\n",
        "features = np.vstack(caption_df['image_features'])\n",
        "captions = padded_sequences\n",
        "\n",
        "# Split data into training and testing sets\n",
        "train_features, test_features, train_captions, test_captions = train_test_split(features, captions, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "UIyBe_zYi_Bi"
      },
      "id": "UIyBe_zYi_Bi",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Image Captioning Model"
      ],
      "metadata": {
        "id": "NbZuXoogjTMw"
      },
      "id": "NbZuXoogjTMw"
    },
    {
      "cell_type": "code",
      "source": [
        "# Image captioning model\n",
        "# Image feature extractor\n",
        "image_input = Input(shape=(4096,))\n",
        "image_dense = Dropout(0.5)(image_input)\n",
        "image_dense = Dense(256, activation='relu')(image_dense)\n",
        "\n",
        "# Sequence processor\n",
        "caption_input = Input(shape=(max_length,))\n",
        "caption_embedding = Embedding(vocab_size, 256, mask_zero=True)(caption_input)\n",
        "caption_lstm = Dropout(0.5)(caption_embedding)\n",
        "caption_lstm = LSTM(256)(caption_lstm)\n",
        "\n",
        "# Decoder (feed both outputs into a single model)\n",
        "decoder = add([image_dense, caption_lstm])\n",
        "decoder = Dense(256, activation='relu')(decoder)\n",
        "outputs = Dense(vocab_size, activation='softmax')(decoder)\n",
        "\n",
        "# Compile the model\n",
        "model = Model(inputs=[image_input, caption_input], outputs=outputs)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8j5ZhEq7jWI3",
        "outputId": "16d3d4f2-f3e3-4296-b43a-95989e83952d"
      },
      "id": "8j5ZhEq7jWI3",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)        [(None, 100)]                0         []                            \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)        [(None, 4096)]               0         []                            \n",
            "                                                                                                  \n",
            " embedding (Embedding)       (None, 100, 256)             999424    ['input_3[0][0]']             \n",
            "                                                                                                  \n",
            " dropout (Dropout)           (None, 4096)                 0         ['input_2[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)         (None, 100, 256)             0         ['embedding[0][0]']           \n",
            "                                                                                                  \n",
            " dense (Dense)               (None, 256)                  1048832   ['dropout[0][0]']             \n",
            "                                                                                                  \n",
            " lstm (LSTM)                 (None, 256)                  525312    ['dropout_1[0][0]']           \n",
            "                                                                                                  \n",
            " add (Add)                   (None, 256)                  0         ['dense[0][0]',               \n",
            "                                                                     'lstm[0][0]']                \n",
            "                                                                                                  \n",
            " dense_1 (Dense)             (None, 256)                  65792     ['add[0][0]']                 \n",
            "                                                                                                  \n",
            " dense_2 (Dense)             (None, 3904)                 1003328   ['dense_1[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 3642688 (13.90 MB)\n",
            "Trainable params: 3642688 (13.90 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train the Model"
      ],
      "metadata": {
        "id": "qq5dbv62khn2"
      },
      "id": "qq5dbv62khn2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Define the parameters"
      ],
      "metadata": {
        "id": "9noYOGB9kmvX"
      },
      "id": "9noYOGB9kmvX"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define parameters\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "steps = len(train_features) // batch_size"
      ],
      "metadata": {
        "id": "g8gJ2lbakv7u"
      },
      "id": "g8gJ2lbakv7u",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Creating data generators"
      ],
      "metadata": {
        "id": "5RiXD7Awk0XQ"
      },
      "id": "5RiXD7Awk0XQ"
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare data generators\n",
        "def data_generator(features, captions, batch_size):\n",
        "    while True:\n",
        "        for i in range(0, len(features), batch_size):\n",
        "            batch_features = features[i:i+batch_size]\n",
        "            batch_captions = captions[i:i+batch_size]\n",
        "            X1, X2, y = [], [], []\n",
        "            for j in range(len(batch_features)):\n",
        "                seq = batch_captions[j]\n",
        "                for k in range(1, len(seq)):\n",
        "                    in_seq, out_seq = seq[:k], seq[k]\n",
        "                    in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
        "                    out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
        "                    X1.append(batch_features[j])\n",
        "                    X2.append(in_seq)\n",
        "                    y.append(out_seq)\n",
        "            yield [np.array(X1), np.array(X2)], np.array(y)"
      ],
      "metadata": {
        "id": "GJK9o6bdk8yZ"
      },
      "id": "GJK9o6bdk8yZ",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create train-test data generators\n",
        "train_generator = data_generator(train_features, train_captions, batch_size)\n",
        "test_generator = data_generator(test_features, test_captions, batch_size)"
      ],
      "metadata": {
        "id": "P58kwhTVlF2l"
      },
      "id": "P58kwhTVlF2l",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model Fit"
      ],
      "metadata": {
        "id": "yC9JtOkXldYy"
      },
      "id": "yC9JtOkXldYy"
    },
    {
      "cell_type": "code",
      "source": [
        " model.fit(train_generator, epochs=epochs, steps_per_epoch=steps, validation_data=test_generator, validation_steps=len(test_features) // batch_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SAzBlJJClg9N",
        "outputId": "f6d285cf-182e-4963-f74f-9b1a6899b496"
      },
      "id": "SAzBlJJClg9N",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "10/10 [==============================] - 20s 1s/step - loss: 6.3729 - val_loss: 5.4164\n",
            "Epoch 2/10\n",
            "10/10 [==============================] - 7s 737ms/step - loss: 5.2645 - val_loss: 5.3160\n",
            "Epoch 3/10\n",
            "10/10 [==============================] - 8s 780ms/step - loss: 5.0707 - val_loss: 5.0397\n",
            "Epoch 4/10\n",
            "10/10 [==============================] - 7s 694ms/step - loss: 4.9601 - val_loss: 5.1828\n",
            "Epoch 5/10\n",
            "10/10 [==============================] - 8s 801ms/step - loss: 4.8569 - val_loss: 5.0034\n",
            "Epoch 6/10\n",
            "10/10 [==============================] - 7s 698ms/step - loss: 4.7772 - val_loss: 5.1245\n",
            "Epoch 7/10\n",
            "10/10 [==============================] - 8s 841ms/step - loss: 4.6350 - val_loss: 5.1443\n",
            "Epoch 8/10\n",
            "10/10 [==============================] - 7s 699ms/step - loss: 4.5423 - val_loss: 5.0061\n",
            "Epoch 9/10\n",
            "10/10 [==============================] - 8s 771ms/step - loss: 4.4562 - val_loss: 5.1643\n",
            "Epoch 10/10\n",
            "10/10 [==============================] - 7s 694ms/step - loss: 4.4294 - val_loss: 5.1188\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7b879046db40>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save the trained model and tokenizer"
      ],
      "metadata": {
        "id": "XgVT2F-xp5yc"
      },
      "id": "XgVT2F-xp5yc"
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the trained model\n",
        "model.save('/content/drive/MyDrive/NLP/model.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1bigaI5dp9ih",
        "outputId": "1c59fa7c-d657-4f28-cb0c-a0354245f4a6"
      },
      "id": "1bigaI5dp9ih",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the tokenizer to a JSON file\n",
        "tokenizer_json = tokenizer.to_json()\n",
        "with open('/content/drive/MyDrive/NLP/tokenizer.json', 'w') as json_file:\n",
        "    json_file.write(tokenizer_json)"
      ],
      "metadata": {
        "id": "unB7mbEMr-Yv"
      },
      "id": "unB7mbEMr-Yv",
      "execution_count": 15,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}