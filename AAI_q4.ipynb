{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d252dbbf-717c-4660-81db-0c458dd13447",
   "metadata": {},
   "source": [
    "# Application of Artificial Intelligence Q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ea1373f-f33a-4e5a-a7e2-b530f7d4f497",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e1452d-5b73-4a08-a3cd-ddb6cb7f45ae",
   "metadata": {},
   "source": [
    "#### Setup the dataset and annotation directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07e553a5-e82a-457e-b644-7a1cb3395bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base directory:  C:\\Users\\admin\\Documents\\ISB AMPBA\\Residency 5\\Artificial Intelligence\\Assignment\n",
      "Data directory:  C:\\Users\\admin\\Documents\\ISB AMPBA\\Residency 5\\Artificial Intelligence\\Assignment\\dataset2\n",
      "Image directory:  C:\\Users\\admin\\Documents\\ISB AMPBA\\Residency 5\\Artificial Intelligence\\Assignment\\dataset2\\images\n",
      "Annotations directory:  C:\\Users\\admin\\Documents\\ISB AMPBA\\Residency 5\\Artificial Intelligence\\Assignment\\dataset2\\annotations\n"
     ]
    }
   ],
   "source": [
    "base_dir = os.getcwd()\n",
    "print(\"Base directory: \", base_dir)\n",
    "data_dir = os.path.join(base_dir, 'dataset2')\n",
    "print(\"Data directory: \", data_dir)\n",
    "image_dir = os.path.join(data_dir, 'images')\n",
    "print(\"Image directory: \", image_dir)\n",
    "annotations_dir = os.path.join(data_dir, 'annotations')\n",
    "print(\"Annotations directory: \", annotations_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6c7261-2177-484f-a331-8fc6df9483b9",
   "metadata": {},
   "source": [
    "#### Create the Test and Train set (100-28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e2fa079-b68b-4cac-8c80-b27ff79b2bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into train (100) and test (28)\n",
    "all_images = os.listdir(image_dir)\n",
    "train_images = all_images[:100]\n",
    "test_images = all_images[100:]\n",
    "\n",
    "## annotation files will be read when the train-test images are processed. It is easier to tag."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1e8cef-5e38-4518-a0ac-6005b26f8ec2",
   "metadata": {},
   "source": [
    "### Load and Transform the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cbbb785-67df-417d-9877-8ede917e9df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.transforms import functional as F\n",
    "from torchvision import transforms as T\n",
    "from PIL import Image\n",
    "import xml.etree.ElementTree as ET\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde3828a-962e-4b35-bbe1-7ae14a5cffd2",
   "metadata": {},
   "source": [
    "#### Setting up a way to read the annotation file and set inline with COCO annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e083d73-75b3-4df1-857c-d196d4dd2de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'boxes': [[72, 150, 172, 399], [192, 229, 308, 534], [324, 246, 415, 410]], 'labels': [4, 3, 1]}\n"
     ]
    }
   ],
   "source": [
    "def parse_voc_annotation(annotation_path):\n",
    "    tree = ET.parse(annotation_path)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    boxes = []\n",
    "    labels = []\n",
    "    for obj in root.findall('object'):\n",
    "        name = obj.find('name').text\n",
    "        if name == 'obj1':\n",
    "            label = 1\n",
    "        elif name == 'obj2':\n",
    "            label = 2\n",
    "        elif name == 'obj3':\n",
    "            label = 3\n",
    "        elif name == 'obj4':\n",
    "            label = 4\n",
    "        \n",
    "        bndbox = obj.find('bndbox')\n",
    "        xmin = int(bndbox.find('xmin').text)\n",
    "        ymin = int(bndbox.find('ymin').text)\n",
    "        xmax = int(bndbox.find('xmax').text)\n",
    "        ymax = int(bndbox.find('ymax').text)\n",
    "        \n",
    "        boxes.append([xmin, ymin, xmax, ymax])\n",
    "        labels.append(label)\n",
    "    \n",
    "    return {'boxes': boxes, 'labels': labels}\n",
    "\n",
    "# Example usage\n",
    "example_path = os.path.join(annotations_dir, '33.xml')  ## has 3 objects\n",
    "parsed_annotation = parse_voc_annotation(example_path)\n",
    "print(parsed_annotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "030c4b91-1c17-47f8-a3b9-a2e2339c7ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_dir, annotation_dir, image_list, transforms=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.annotation_dir = annotation_dir\n",
    "        self.image_list = image_list\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.image_dir, self.image_list[idx])\n",
    "        annotation_path = os.path.join(self.annotation_dir, self.image_list[idx].replace('.jpg', '.xml'))\n",
    "        \n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        annot = parse_voc_annotation(annotation_path)\n",
    "        \n",
    "        boxes = torch.as_tensor(annot['boxes'], dtype=torch.float32)\n",
    "        labels = torch.as_tensor(annot['labels'], dtype=torch.int64)\n",
    "        # Generate a unique image_id based on idx or filename\n",
    "        image_id = idx  # Example: using idx as image_id\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)\n",
    "\n",
    "        target = {\n",
    "            'boxes': boxes,\n",
    "            'labels': labels,\n",
    "            'image_id': image_id,\n",
    "            'area': area,\n",
    "            'iscrowd': iscrowd\n",
    "        }\n",
    "\n",
    "        # Apply transformations to the image only\n",
    "        if self.transforms:\n",
    "            img = self.transforms(img)\n",
    "        \n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_list)\n",
    "\n",
    "# Transformations\n",
    "transform = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.RandomHorizontalFlip(0.5)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4709bc5-8bc3-43ae-9a80-423a7698414d",
   "metadata": {},
   "source": [
    "#### Creating the datasets and Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7ffd4f2-61a4-4c2f-9e1c-980f3c72f2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "train_dataset = CustomDataset(image_dir, annotations_dir, train_images, transform)\n",
    "test_dataset = CustomDataset(image_dir, annotations_dir, test_images, transform)\n",
    "\n",
    "# Data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=2, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae81dfb-71e6-4474-8e1d-28fd8db2881e",
   "metadata": {},
   "source": [
    "#### Load the Pretrained Model\n",
    "Load a pretrained Faster R-CNN model and modify it to fit our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cbe14ba3-038b-4210-bd76-cf6de22be4c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\admin\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "def get_model_instance_segmentation(num_classes):\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    return model\n",
    "\n",
    "num_classes = 5  # 4 classes (Water Bottle, Milk Bottle, Tetra Pack, Can) + background\n",
    "model = get_model_instance_segmentation(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "248146bb-9829-44bd-9a88-96db4050fdd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Supporting python files for torchvision and coco\n",
    "from engine_detection import train_one_epoch, evaluate\n",
    "import utils_detection as utils\n",
    "from coco_utils_detection import get_coco_api_from_dataset\n",
    "from coco_eval_detection import CocoEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5328d4a5-3b20-4556-9c16-549b83af23da",
   "metadata": {},
   "source": [
    "#### Fine tuning the FastRCNN model with our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e84808-9f51-4e9a-89e9-e988450cc416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0]  [ 0/50]  eta: 0:43:50  lr: 0.000107  loss: 2.1990 (2.1990)  loss_classifier: 1.9727 (1.9727)  loss_box_reg: 0.2160 (0.2160)  loss_objectness: 0.0018 (0.0018)  loss_rpn_box_reg: 0.0084 (0.0084)  time: 52.6128  data: 0.2190\n",
      "Epoch: [0]  [10/50]  eta: 0:34:36  lr: 0.001126  loss: 1.1907 (1.3750)  loss_classifier: 0.9729 (1.0818)  loss_box_reg: 0.2703 (0.2658)  loss_objectness: 0.0171 (0.0177)  loss_rpn_box_reg: 0.0084 (0.0097)  time: 51.9122  data: 0.2629\n",
      "Epoch: [0]  [20/50]  eta: 0:24:31  lr: 0.002146  loss: 0.7239 (1.0505)  loss_classifier: 0.4186 (0.7490)  loss_box_reg: 0.2703 (0.2711)  loss_objectness: 0.0099 (0.0214)  loss_rpn_box_reg: 0.0078 (0.0090)  time: 48.8638  data: 0.5069\n",
      "Epoch: [0]  [30/50]  eta: 0:15:07  lr: 0.003165  loss: 0.5157 (0.8722)  loss_classifier: 0.2851 (0.5799)  loss_box_reg: 0.2339 (0.2614)  loss_objectness: 0.0118 (0.0214)  loss_rpn_box_reg: 0.0092 (0.0094)  time: 41.7812  data: 0.9619\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "num_classes = 5  # 4 classes + background\n",
    "model = get_model_instance_segmentation(num_classes)\n",
    "\n",
    "# Move model to the right device\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Construct an optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# Learning rate scheduler\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_one_epoch(model, optimizer, train_loader, device, epoch, print_freq=10)\n",
    "    lr_scheduler.step()\n",
    "    evaluate(model, test_loader, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5210eb6e-7646-4a69-9585-bbd380ae7e21",
   "metadata": {},
   "source": [
    "#### Evaluating the model on 28 Test images\n",
    "We are using the following metrics with an IoU threshold of 0.5\n",
    "##### Precision\n",
    "##### Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbba0110-f334-4d7b-9e86-9fe25dc8ac0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.ops import box_iou\n",
    "\n",
    "def calculate_precision_recall(model, data_loader, device, iou_threshold=0.5):\n",
    "    model.eval()\n",
    "    true_positives = {i: 0 for i in range(1, num_classes)}\n",
    "    false_positives = {i: 0 for i in range(1, num_classes)}\n",
    "    false_negatives = {i: 0 for i in range(1, num_classes)}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, targets in data_loader:\n",
    "            images = list(img.to(device) for img in images)\n",
    "            outputs = model(images)\n",
    "            \n",
    "            for target, output in zip(targets, outputs):\n",
    "                gt_boxes = target['boxes'].cpu()\n",
    "                gt_labels = target['labels'].cpu()\n",
    "                pred_boxes = output['boxes'].cpu()\n",
    "                pred_labels = output['labels'].cpu()\n",
    "                pred_scores = output['scores'].cpu()\n",
    "                \n",
    "                for label in range(1, num_classes):\n",
    "                    gt_mask = gt_labels == label\n",
    "                    pred_mask = pred_labels == label\n",
    "                    \n",
    "                    gt_boxes_label = gt_boxes[gt_mask]\n",
    "                    pred_boxes_label = pred_boxes[pred_mask]\n",
    "                    pred_scores_label = pred_scores[pred_mask]\n",
    "                    \n",
    "                    if len(pred_boxes_label) == 0:\n",
    "                        false_negatives[label] += len(gt_boxes_label)\n",
    "                        continue\n",
    "                    \n",
    "                    ious = box_iou(gt_boxes_label, pred_boxes_label)\n",
    "                    max_iou, _ = ious.max(dim=1)\n",
    "                    matched_pred_indices = max_iou >= iou_threshold\n",
    "                    \n",
    "                    true_positives[label] += matched_pred_indices.sum().item()\n",
    "                    false_positives[label] += (~matched_pred_indices).sum().item()\n",
    "                    false_negatives[label] += (gt_mask.sum().item() - matched_pred_indices.sum().item())\n",
    "    \n",
    "    precision = {label: true_positives[label] / (true_positives[label] + false_positives[label]) if (true_positives[label] + false_positives[label]) > 0 else 0.0 for label in range(1, num_classes)}\n",
    "    recall = {label: true_positives[label] / (true_positives[label] + false_negatives[label]) if (true_positives[label] + false_negatives[label]) > 0 else 0.0 for label in range(1, num_classes)}\n",
    "    \n",
    "    return precision, recall\n",
    "\n",
    "precision, recall = calculate_precision_recall(model, test_loader, device, iou_threshold=0.5)\n",
    "\n",
    "for label in range(1, num_classes):\n",
    "    print(f'Class {label}: Precision = {precision[label]:.2f}, Recall = {recall[label]:.2f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
